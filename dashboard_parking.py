import streamlit as st
import requests
import re
import pandas as pd
from datetime import datetime
import threading
import time
import json
import os
from zoneinfo import ZoneInfo

st.set_page_config(
    page_title="Parkings Aix-en-Provence",
    page_icon="ğŸ…¿ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.title("ğŸ…¿ï¸ Parkings Aix-en-Provence")
st.subheader("Places disponibles en temps rÃ©el")

parkings = {
    'Bellegarde': ('https://mamp.parkings-semepa.fr/', 213),
    'Cardeurs': ('https://mamp.parkings-semepa.fr/', 219),
    'Carnot': ('https://mamp.parkings-semepa.fr/', 211),
    'MÃ©janes': ('https://mamp.parkings-semepa.fr/', 150),
    'Mignet': ('https://mamp.parkings-semepa.fr/', 209),
    'Pasteur': ('https://mamp.parkings-semepa.fr/', 215),
    'Rambot': ('https://parkings-semepa.fr/', 221),
    'Rotonde': ('https://parkings-semepa.fr/', 206),
    'Signoret': ('https://mamp.parkings-semepa.fr/', 217)
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# Fichier de cache pour les donnÃ©es
CACHE_FILE = 'parkings_cache.json'
LOCK_FILE = 'scraping.lock'

def load_cache():
    """Charge les donnÃ©es du cache"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_cache(data):
    """Sauvegarde les donnÃ©es dans le cache"""
    with open(CACHE_FILE, 'w') as f:
        json.dump(data, f)

def scraper_parkings():
    """Scrape tous les parkings"""
    data = {}
    
    for nom, (base_url, page_id) in parkings.items():
        try:
            response = requests.get(base_url, params={"page_id": page_id}, headers=headers, timeout=5)
            
            match_nombre = re.search(r'<p class="nbPlaces"><span[^>]*>(\d+)</span>', response.text)
            match_texte = re.search(r'<p class="nbPlaces"><span[^>]*>([^<]+)</span>', response.text)
            
            if match_nombre:
                places_libres = int(match_nombre.group(1))
                data[nom] = {
                    'Places': places_libres,
                    'Statut': 'âœ… Ouvert',
                    'Timestamp': datetime.now(ZoneInfo("Europe/Paris")).strftime("%H:%M:%S")
                }
            elif match_texte:
                statut = match_texte.group(1)
                data[nom] = {
                    'Places': 0,
                    'Statut': f'âš ï¸ {statut}',
                    'Timestamp': datetime.now(ZoneInfo("Europe/Paris")).strftime("%H:%M:%S")
                }
            else:
                data[nom] = {
                    'Places': 0,
                    'Statut': 'â“ Pas de donnÃ©es',
                    'Timestamp': datetime.now(ZoneInfo("Europe/Paris")).strftime("%H:%M:%S")
                }
            
        except Exception as e:
            data[nom] = {
                'Places': 0,
                'Statut': f'âŒ Erreur',
                'Timestamp': datetime.now(ZoneInfo("Europe/Paris")).strftime("%H:%M:%S")
            }
        
        time.sleep(0.5)
    
    return data

def scraper_background():
    """Fonction qui scrape en arriÃ¨re-plan toutes les 30 mins"""
    while True:
        try:
            now = datetime.now(ZoneInfo("Europe/Paris")).strftime("%d/%m/%Y %H:%M:%S")
            print(f"[{now}] Scraping en arriÃ¨re-plan...")
            data = scraper_parkings()
            save_cache(data)
            now = datetime.now(ZoneInfo("Europe/Paris")).strftime("%d/%m/%Y %H:%M:%S")
            print(f"[{now}] Scraping terminÃ© et cache mis Ã  jour")
        except Exception as e:
            print(f"Erreur lors du scraping: {e}")
        
        # Attendre 10 minutes (600 secondes)
        time.sleep(600)

# Initialiser le thread de scraping en background au dÃ©marrage
if 'scraper_started' not in st.session_state:
    scraper_thread = threading.Thread(target=scraper_background, daemon=True)
    scraper_thread.start()
    st.session_state.scraper_started = True
    print("ğŸš€ Thread de scraping lancÃ© en background")

# Charger les donnÃ©es du cache
cached_data = load_cache()

if not cached_data:
    st.info("ğŸ”„ PremiÃ¨re initialisation... chargement des donnÃ©es...")
    with st.spinner("RÃ©cupÃ©ration des donnÃ©es en cours..."):
        cached_data = scraper_parkings()
        save_cache(cached_data)
    st.success("âœ… Chargement des donnÃ©es terminÃ©!")

# Afficher bouton pour forcer la mise Ã  jour
col1, col2, col3 = st.columns([1, 1, 1])
with col2:
    if st.button("ğŸ”„ RafraÃ®chir maintenant", use_container_width=True):
        with st.spinner("RÃ©cupÃ©ration des donnÃ©es..."):
            cached_data = scraper_parkings()
            save_cache(cached_data)
        st.success("âœ… DonnÃ©es mises Ã  jour!")
        st.rerun()

# Convertir en DataFrame et trier
df = pd.DataFrame(cached_data).T
df = df.sort_values('Places', ascending=False)

# RÃ©cupÃ©rer le dernier timestamp (tous les parkings ont le mÃªme)
last_update = df['Timestamp'].iloc[0] if len(df) > 0 else "N/A"

# Afficher les stats globales
col1, col2, col3 = st.columns(3)

with col1:
    total_places = df['Places'].sum()
    st.metric("Total places", total_places, delta="places disponibles")

with col2:
    open_count = len(df[df['Statut'] == 'âœ… Ouvert'])
    st.metric("Parkings ouverts", f"{open_count}/9")

with col3:
    st.metric("DerniÃ¨re mise Ã  jour", last_update)
    st.caption(f"ğŸ• {datetime.now(ZoneInfo('Europe/Paris')).strftime('%d/%m/%Y')}")

st.divider()

# Afficher les cards
cols = st.columns(3)

for idx, (nom, row) in enumerate(df.iterrows()):
    col = cols[idx % 3]
    
    with col:
        if row['Statut'] == 'âœ… Ouvert':
            container = st.container(border=True)
            container.metric(nom, f"{int(row['Places'])} places", delta=row['Statut'])
            container.caption(f"ğŸ• {row['Timestamp']}")
        else:
            container = st.container(border=True)
            container.warning(f"**{nom}**\n\n{row['Statut']}")
            container.caption(f"ğŸ• {row['Timestamp']}")

st.divider()

# Tableau dÃ©taillÃ©
st.subheader("ğŸ“Š Tableau dÃ©taillÃ©")
st.dataframe(df, use_container_width=True)

st.caption("â±ï¸ Scraping automatique en arriÃ¨re-plan toutes les 10 minutes")